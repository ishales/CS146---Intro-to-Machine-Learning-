{"cells":[{"cell_type":"markdown","metadata":{"id":"_WLVhlv99E-h"},"source":["# Problem 1: A Two-Layer Neural Network for Binary Classification"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jqQZ3TWY9E-i"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import os\n","import gzip\n","import matplotlib.pyplot as plt\n","from sklearn.cluster import KMeans\n","from tqdm import tqdm\n","\n","# Load matplotlib images inline\n","%matplotlib inline\n","# These are important for reloading any code you write in external .py files.\n","# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n","%load_ext autoreload\n","%autoreload 2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MRqR8CZ39E-j"},"outputs":[],"source":["## Load fashionMNIST. This is the same code with homework 1.\n","##\n","def crop_center(img,cropped):\n","    img = img.reshape(-1, 28, 28)\n","    start = 28//2-(cropped//2)\n","    img = img[:, start:start+cropped, start:start+cropped]\n","    return img.reshape(-1, cropped*cropped)\n","\n","def load_mnist(path, kind='train'):\n","\n","    \"\"\"Load MNIST data from `path`\"\"\"\n","    labels_path = os.path.join(path,'%s-labels-idx1-ubyte.gz' % kind)\n","    images_path = os.path.join(path, '%s-images-idx3-ubyte.gz'% kind)\n","\n","    with gzip.open(labels_path, 'rb') as lbpath:\n","        labels = np.frombuffer(lbpath.read(), 'B', offset=8)\n","\n","    with gzip.open(images_path, 'rb') as imgpath:\n","        images = np.frombuffer(imgpath.read(),'B', offset=16).reshape(-1, 784)\n","        images = crop_center(images, 24)\n","    return images, labels\n","X_train_and_val, y_train_and_val = load_mnist('./data/mnist', kind='train')\n","X_test, y_test = load_mnist('./data/mnist', kind='t10k')\n","X_train, X_val = X_train_and_val[:50000], X_train_and_val[50000:]\n","y_train, y_val = y_train_and_val[:50000], y_train_and_val[50000:]\n","print('Train data shape: ', X_train.shape)\n","print('Train target shape: ', y_train.shape)\n","print('Val data shape: ', X_val.shape)\n","print('Val target shape: ', y_val.shape)\n","print('Test data shape: ',X_test.shape)\n","print('Test target shape: ',y_test.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xjfsY8Ox9E-j"},"outputs":[],"source":["# PART (a):\n","# To Visualize a point in the dataset\n","index = 10\n","X = np.array(X_train[index], dtype='uint8').reshape([24, 24])\n","fig = plt.figure()\n","plt.imshow(X, cmap='gray')\n","plt.show()\n","if y_train[index] in set([1, 3, 5, 7, 9]):\n","    label = 'Odd'\n","else:\n","    label = 'Even'\n","print('Label is', label)"]},{"cell_type":"markdown","metadata":{"id":"6ozhJL4T9E-j"},"source":["In the following cells, you will build a two-layer neural network."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mo9zmasa9E-j"},"outputs":[],"source":["# convert to binary label\n","y_train = y_train.astype(int) % 2\n","y_val = y_val.astype(int) % 2\n","y_test = y_test.astype(int) % 2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WzU3563s9E-j"},"outputs":[],"source":["class TwoLayerNet(object):\n","    \"\"\"\n","    A two-layer fully-connected neural network for binary classification.\n","    We train the network with a softmax output and cross entropy loss function\n","    with L2 regularization on the weight matrices. The network uses a ReLU\n","    nonlinearity after the first fully connected layer.\n","    Input: X\n","    Hidden states for layer 1: h1 = XW1 + b1\n","    Activations: a1 = ReLU(h1)\n","    Hidden states for layer 2: h2 = a1W2 + b2\n","    Probabilities: s = softmax(h2)\n","\n","    ReLU function:\n","    (i) x = x if x >= 0  (ii) x = 0 if x < 0\n","\n","    The outputs of the second fully-connected layer are the scores for each class.\n","    \"\"\"\n","\n","    def __init__(self, input_size, hidden_size, output_size, std=1e-4):\n","        \"\"\"\n","        Initialize the model. Weights are initialized to small random values and\n","        biases are initialized to zero. Weights and biases are stored in the\n","        variable self.params, which is a dictionary with the following keys:\n","\n","        W1: First layer weights; has shape (D, H)\n","        b1: First layer biases; has shape (H,)\n","        W2: Second layer weights; has shape (H, C)\n","        b2: Second layer biases; has shape (C,)\n","\n","        Inputs:\n","        - input_size: The dimension D of the input data.\n","        - hidden_size: The number of neurons H in the hidden layer.\n","        - output_size: The number of classes C.\n","        \"\"\"\n","        self.params = {}\n","        self.params['W1'] = std * np.random.randn(input_size, hidden_size)\n","        self.params['b1'] = np.zeros(hidden_size)\n","        self.params['W2'] = std * np.random.randn(hidden_size, output_size)\n","        self.params['b2'] = np.zeros(output_size)\n","\n","    def loss(self, X, y=None, reg=0.0):\n","        \"\"\"\n","        Compute the loss and gradients for a two layer fully connected neural\n","        network.\n","\n","        Inputs:\n","        - X: Input data of shape (N, D). Each X[i] is a training sample.\n","        - y: Vector of training labels. y[i] is the label for X[i], and each y[i] is\n","          an integer in the range 0 <= y[i] < C. This parameter is optional; if it\n","          is not passed then we only return scores, and if it is passed then we\n","          instead return the loss and gradients.\n","        - reg: Regularization strength.\n","\n","        Returns:\n","        If y is None, return a matrix scores of shape (N, C) where scores[i, c] is\n","        the score for class c on input X[i].\n","\n","        If y is not None, instead return a tuple of:\n","        - loss: Loss (data loss and regularization loss) for this batch of training\n","          samples.\n","        - grads: Dictionary mapping parameter names to gradients of those parameters\n","          with respect to the loss function; has the same keys as self.params.\n","        \"\"\"\n","        # Unpack variables from the params dictionary\n","        W1, b1 = self.params['W1'], self.params['b1']\n","        W2, b2 = self.params['W2'], self.params['b2']\n","        N, D = X.shape\n","\n","        # Compute the forward pass\n","        scores = None\n","\n","        ### ========== TODO : START ========== ###\n","        #   Calculate the output of the neural network using forward pass.\n","        #   The expected result should be a matrix of shape (N, C), where:\n","        #     - N is the number of examples in the input dataset 'X'.\n","        #     - C is the number of classes.\n","        #   Use 'h1' as the first hidden layer output\n","        #   Apply the ReLU activation function to 'h1' to get 'a1'. Use np.maximum for ReLU implementation.\n","        #   The output 'scores' is the result of the second layer (before applying softmax).\n","        #   Refer to the model architecture comments at the beginning of this class for more details.\n","        #   Note: Do not use a for loop in your implementation.\n","        ##  Part (b): Implement the forward pass and compute scores.\n","\n","        ### ========== TODO : END ========== ###\n","\n","\n","        # If the targets are not given then jump out, we're done\n","        if y is None:\n","            return scores\n","\n","        # Compute the loss\n","        loss = None\n","\n","        # scores is num_examples by num_classes (N, C)\n","        def softmax_loss(x, y):\n","            ### ========== TODO : START ========== ###\n","            #   Calculate the cross entropy loss after softmax output layer.\n","            #   This function should return loss and dx\n","            probs = np.exp(x - np.max(x, axis=1, keepdims=True)) # Other Notes: this operation is called stable softmax: numerically more stable as it reduces overflow issues by not letting the numerator and denominator grow too big.\n","            probs /= np.sum(probs, axis=1, keepdims=True)\n","            N = x.shape[0]\n","            ##  Part (d): Implement the CrossEntropyLoss\n","            loss = None\n","            ##  Part (d): Implement the gradient of y wrt x\n","            dx = None\n","\n","            ### ========== TODO : END ========== ###\n","            return loss, dx\n","\n","\n","        data_loss, dscore = softmax_loss(scores, y)\n","\n","        ### ========== TODO : START ========== ###\n","        #   Calculate the regularization loss. Multiply the regularization\n","        #   loss by 0.5 (in addition to the regularization factor 'reg').\n","        ##  Part (c): Implement the regularization loss\n","        reg_loss = None\n","        ### ========== TODO : END ========== ###\n","\n","        loss = data_loss + reg_loss\n","\n","        grads = {}\n","\n","        ### ========== TODO : START ========== ###\n","        #  Compute backpropagation\n","        #  Remember the loss contains two parts: cross-entropy and regularization. The computation for gradients of W1 and b1 shown here can be regarded as a reference.\n","        ## Part (e): Implement the computations of gradients for W2 and b2.\n","        grads['W2'] = None\n","        grads['b2'] = None\n","\n","        dh = np.dot(dscore, W2.T)\n","        dh[a1 <= 0] = 0\n","\n","        grads['W1'] = np.dot(X.T, dh) + reg * W1\n","        grads['b1'] = np.ones(N).dot(dh)\n","        ### ========== TODO : END ========== ###\n","\n","        return loss, grads\n","\n","    def train(self, X, y, X_val, y_val,\n","            learning_rate=1e-3, learning_rate_decay=0.95,\n","            reg=1e-5, num_iters=100,\n","            batch_size=200, verbose=False):\n","        \"\"\"\n","        Train this neural network using stochastic gradient descent.\n","\n","        Inputs:\n","        - X: A numpy array of shape (N, D) giving training data.\n","        - y: A numpy array f shape (N,) giving training labels; y[i] = c means that\n","          X[i] has label c, where 0 <= c < C.\n","        - X_val: A numpy array of shape (N_val, D) giving validation data.\n","        - y_val: A numpy array of shape (N_val,) giving validation labels.\n","        - learning_rate: Scalar giving learning rate for optimization.\n","        - learning_rate_decay: Scalar giving factor used to decay the learning rate\n","          after each epoch.\n","        - reg: Scalar giving regularization strength.\n","        - num_iters: Number of steps to take when optimizing.\n","        - batch_size: Number of training examples to use per step.\n","        - verbose: boolean; if true print progress during optimization.\n","        \"\"\"\n","        num_train = X.shape[0]\n","        iterations_per_epoch = max(num_train / batch_size, 1)\n","\n","        # Use SGD to optimize the parameters in self.model\n","        loss_history = []\n","        train_acc_history = []\n","        val_acc_history = []\n","\n","        for it in np.arange(num_iters):\n","            X_batch = None\n","            y_batch = None\n","\n","            #   Create a minibatch (X_batch, y_batch) by sampling batch_size\n","            #   samples randomly.\n","\n","            b_index = np.random.choice(num_train, batch_size)\n","            X_batch = X[b_index]\n","            y_batch = y[b_index]\n","\n","            # Compute loss and gradients using the current minibatch\n","            loss, grads = self.loss(X_batch, y=y_batch, reg=reg)\n","            loss_history.append(loss)\n","\n","\n","            self.params['W1'] -= learning_rate * grads['W1']\n","            self.params['b1'] -= learning_rate * grads['b1']\n","            self.params['W2'] -= learning_rate * grads['W2']\n","            self.params['b2'] -= learning_rate * grads['b2']\n","\n","\n","            if verbose and it % 100 == 0:\n","                print('iteration {} / {}: loss {}'.format(it, num_iters, loss))\n","\n","            # Every epoch, check train and val accuracy and decay learning rate.\n","            if it % iterations_per_epoch == 0:\n","                # Check accuracy\n","                train_acc = (self.predict(X_batch) == y_batch).mean()\n","                val_acc = (self.predict(X_val) == y_val).mean()\n","                train_acc_history.append(train_acc)\n","                val_acc_history.append(val_acc)\n","\n","                # Decay learning rate\n","                learning_rate *= learning_rate_decay\n","\n","        return {\n","          'loss_history': loss_history,\n","          'train_acc_history': train_acc_history,\n","          'val_acc_history': val_acc_history,\n","        }\n","\n","    def predict(self, X):\n","        \"\"\"\n","        Use the trained weights of this two-layer network to predict labels for\n","        data points. For each data point we predict scores for each of the C\n","        classes, and assign each data point to the class with the highest score.\n","\n","        Inputs:\n","        - X: A numpy array of shape (N, D) giving N D-dimensional data points to\n","          classify.\n","\n","        Returns:\n","        - y_pred: A numpy array of shape (N,) giving predicted labels for each of\n","          the elements of X. For all i, y_pred[i] = c means that X[i] is predicted\n","          to have class c, where 0 <= c < C.\n","        \"\"\"\n","        y_pred = None\n","\n","        ### ========== TODO : START ========== ###\n","        #   Predict the class given the input data.\n","        ##  Part (f): Implement the prediction function\n","\n","        ### ========== TODO : END ========== ###\n","\n","        return y_pred"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O1NPs0Jy9E-k"},"outputs":[],"source":["input_size = 576\n","hidden_size = 50\n","num_classes = 2\n","net = TwoLayerNet(input_size, hidden_size, num_classes)\n","\n","# Train the network\n","for learning_rate in [1e-5, 1e-4, 1e-3, 5e-3, 1e-1]:\n","  print('learning_rate: ', learning_rate)\n","  stats = net.train(X_train, y_train, X_val, y_val,\n","              num_iters=1000, batch_size=200,\n","              learning_rate=learning_rate, learning_rate_decay=0.95,\n","              reg=0.1, verbose=True)\n","\n","  # Predict on the validation set\n","  val_acc = (net.predict(X_val) == y_val).mean()\n","  print('Validation accuracy: ', val_acc)\n","\n","  # Save this net as the variable subopt_net for later comparison.\n","  subopt_net = net\n","  test_acc = (subopt_net.predict(X_test) == y_test).mean()\n","  print('Test accuracy (subopt_net): ', test_acc)\n","  print('\\n')"]},{"cell_type":"markdown","metadata":{"id":"y-RDGlMK9E-k"},"source":["# Problem 2: K-Means Algorithm"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"HXaEq7BY9E-k","executionInfo":{"status":"ok","timestamp":1700340896141,"user_tz":480,"elapsed":5,"user":{"displayName":"SIYAN ZHAO","userId":"02511534054634309772"}}},"outputs":[],"source":["## Function to load the CIFAR10 data\n","## Documentation of CIFAR10: https://www.cs.toronto.edu/~kriz/cifar.html\n","def dataloader():\n","  import tensorflow as tf\n","  cifar10 = tf.keras.datasets.cifar10\n","  (_, _), (X, y) = cifar10.load_data()\n","  return X, y"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5Mvwjg2T9E-k"},"outputs":[],"source":["## simple utility function to visualize the data\n","def visualize(X, ind):\n","  from PIL import Image\n","  plt.imshow(Image.fromarray(X[ind], 'RGB'))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PHg7PX6z9E-k"},"outputs":[],"source":["X, y = dataloader()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2X7wZU6n9E-k"},"outputs":[],"source":["# 10K images of size 32 x 32 x 3\n","# where 32 x 32 is the height and width of the image\n","# 3 is the number of channels 'RGB'\n","X.shape, y.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Cx-yZmli9E-k"},"outputs":[],"source":["visualize(X, 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nPxMMp0H9E-k"},"outputs":[],"source":["'''\n","  Implement this function to form a 10000 x N matrix\n","  from 10000 x 32 x 32 x 3 shape input.\n","'''\n","def reshape(X):\n","  '''\n","    Write one line of code here\n","  '''\n","  ### ========== TODO : START ========== ###\n","  # part (a)\n","\n","  ### ========== TODO : END ========== ###\n","  return X"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZsCg1dGe9E-k"},"outputs":[],"source":["X = reshape(X)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DMO2nNTr9E-k"},"outputs":[],"source":["clustering_score = []\n","for i in tqdm(range(5, 20, 5)):\n","  score = 0\n","  for rs in tqdm(range(3)):\n","    kmeans = KMeans(n_clusters = i, init = 'random', random_state = rs)\n","    '''\n","      Write one line of code to fit the kMeans algorithm to the data\n","      Write another line of code to report the kMeans clustering score\n","      defined as sum of squared distances of samples to their closest\n","      cluster center, weighted by the sample weights if provided.\n","      Hint: https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n","    '''\n","    ### ========== TODO : START ========== ###\n","    # part (b)\n","\n","    ### ========== TODO : END ========== ###\n","  clustering_score.append(score/3) ## divide by 3 because 3 random states"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TYPDISvk9E-k"},"outputs":[],"source":["'''\n","  Submit the plot you get after running this piece of code in your solutions\n","'''\n","plt.figure(figsize=(10,6))\n","plt.plot(range(5, 20, 5), clustering_score)\n","plt.xlabel('No. of Clusters')\n","plt.ylabel('Clustering Score')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"CXX-UYJk9E-k"},"source":["### Visualize K Clusters for K = 10 and random_state = 42"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZSGKi7vd9E-l"},"outputs":[],"source":["from sklearn.decomposition import PCA\n","pca = PCA(2)\n","#Transform the data\n","df = pca.fit_transform(X)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DILDrpjR9E-l"},"outputs":[],"source":["### Analyzing the input data in 2D based on its true labels\n","\n","u_labels = np.unique(y[:, 0])\n","\n","for i in u_labels:\n","    plt.scatter(df[y[:, 0] == i , 0] , df[y[:, 0] == i , 1] , label = i)\n","plt.legend()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JD90h31o9E-l"},"outputs":[],"source":["'''\n","  Submit the output plot as a part of the solutions\n","'''\n","\n","kmeans = KMeans(n_clusters = 10, init = 'random', random_state = 42)\n","'''\n","  Write 1 - 2 line of code to get the predicted labels of the 10-clusters\n","'''\n","### ========== TODO : START ========== ###\n","\n","### ========== TODO : END ========== ###\n","\n","u_labels = np.unique(label)\n","\n","#plotting the results:\n","\n","for i in u_labels:\n","    '''\n","      Write one line of code to get a scatter plot for i-th cluster.\n","      Have its label = i\n","    '''\n","    ### ========== TODO : START ========== ###\n","\n","    ### ========== TODO : END ========== ###\n","\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"rTqbZsZM9E-l"},"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.9"},"orig_nbformat":4,"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}